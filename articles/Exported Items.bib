
@misc{wolf_huggingfaces_2020,
	title = {{HuggingFace}'s {Transformers}: {State}-of-the-art {Natural} {Language} {Processing}},
	shorttitle = {{HuggingFace}'s {Transformers}},
	url = {http://arxiv.org/abs/1910.03771},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered stateof-the art Transformer architectures under a uniﬁed API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/ huggingface/transformers.},
	language = {en},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	month = jul,
	year = {2020},
	note = {arXiv:1910.03771 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 8 pages, 4 figures, more details at https://github.com/huggingface/transformers},
	file = {Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf:/Users/lenatrnovec/Zotero/storage/WYLLLAIA/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natur.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general.},
	language = {en},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:/Users/lenatrnovec/Zotero/storage/G2C9PCVF/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@inproceedings{clark_all_2021,
	address = {Online},
	title = {All {That}’s ‘{Human}’ {Is} {Not} {Gold}: {Evaluating} {Human} {Evaluation} of {Generated} {Text}},
	shorttitle = {All {That}’s ‘{Human}’ {Is} {Not} {Gold}},
	url = {https://aclanthology.org/2021.acl-long.565},
	doi = {10.18653/v1/2021.acl-long.565},
	language = {en},
	urldate = {2023-12-04},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Elizabeth and August, Tal and Serrano, Sofia and Haduong, Nikita and Gururangan, Suchin and Smith, Noah A.},
	year = {2021},
	pages = {7282--7296},
	file = {Clark et al. - 2021 - All That’s ‘Human’ Is Not Gold Evaluating Human E.pdf:/Users/lenatrnovec/Zotero/storage/CXWLIDT6/Clark et al. - 2021 - All That’s ‘Human’ Is Not Gold Evaluating Human E.pdf:application/pdf},
}

@misc{black_gpt-neox-20b_2022,
	title = {{GPT}-{NeoX}-{20B}: {An} {Open}-{Source} {Autoregressive} {Language} {Model}},
	shorttitle = {{GPT}-{NeoX}-{20B}},
	url = {http://arxiv.org/abs/2204.06745},
	abstract = {We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B’s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We ﬁnd that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated ﬁve-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https:// github.com/EleutherAI/gpt-neox.},
	language = {en},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06745 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To appear in the Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
	file = {Black et al. - 2022 - GPT-NeoX-20B An Open-Source Autoregressive Langua.pdf:/Users/lenatrnovec/Zotero/storage/HES6NEPS/Black et al. - 2022 - GPT-NeoX-20B An Open-Source Autoregressive Langua.pdf:application/pdf},
}

@misc{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {http://arxiv.org/abs/1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most inﬂuence end-task performance.},
	language = {en},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
	note = {arXiv:1910.13461 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:/Users/lenatrnovec/Zotero/storage/KT9F4E7A/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf:application/pdf},
}

@misc{ott_fairseq_2019,
	title = {fairseq: {A} {Fast}, {Extensible} {Toolkit} for {Sequence} {Modeling}},
	shorttitle = {fairseq},
	url = {http://arxiv.org/abs/1904.01038},
	abstract = {FAIRSEQ is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found here: https://www.youtube. com/watch?v=OtgDdWtHvto.},
	language = {en},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
	month = apr,
	year = {2019},
	note = {arXiv:1904.01038 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NAACL 2019 Demo paper},
	file = {Ott et al. - 2019 - fairseq A Fast, Extensible Toolkit for Sequence M.pdf:/Users/lenatrnovec/Zotero/storage/GNG6Q44T/Ott et al. - 2019 - fairseq A Fast, Extensible Toolkit for Sequence M.pdf:application/pdf},
}

@misc{ng_facebook_2019,
	title = {Facebook {FAIR}'s {WMT19} {News} {Translation} {Task} {Submission}},
	url = {http://arxiv.org/abs/1907.06616},
	abstract = {This paper describes Facebook FAIR’s submission to the WMT19 shared news translation task. We participate in two language pairs and four language directions, English ↔ German and English ↔ Russian. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit which rely on sampled backtranslations. This year we experiment with different bitext data ﬁltering schemes, as well as with adding ﬁltered back-translated data. We also ensemble and ﬁne-tune our models on domain-speciﬁc data, then decode using noisy channel model reranking. Our submissions are ranked ﬁrst in all four directions of the human evaluation campaign. On En→De, our system signiﬁcantly outperforms other systems as well as human translations. This system improves upon our WMT’18 submission by 4.5 BLEU points.},
	language = {en},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Ng, Nathan and Yee, Kyra and Baevski, Alexei and Ott, Myle and Auli, Michael and Edunov, Sergey},
	month = jul,
	year = {2019},
	note = {arXiv:1907.06616 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 7 pages; WMT},
	file = {Ng et al. - 2019 - Facebook FAIR's WMT19 News Translation Task Submis.pdf:/Users/lenatrnovec/Zotero/storage/KFJR7ZD3/Ng et al. - 2019 - Facebook FAIR's WMT19 News Translation Task Submis.pdf:application/pdf},
}

@article{abadi_tensorflow_nodate,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataﬂow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataﬂow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives ﬂexibility to the application developer: whereas in previous “parameter server” designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataﬂow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
	language = {en},
	author = {Abadi, Martın and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	file = {Abadi et al. - TensorFlow A system for large-scale machine learn.pdf:/Users/lenatrnovec/Zotero/storage/ZZ4FZ337/Abadi et al. - TensorFlow A system for large-scale machine learn.pdf:application/pdf},
}
